{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c537e67-b64a-43a6-bdf1-826953b62316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c390a87-dc2b-4410-96c9-5ba753bdac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : X = (36000, 784) ; y = (36000,)\n",
      "val   : X = (6000, 784) ; y = (6000,)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train, validation = train_test_split(\n",
    "    train, test_size = 1 / 7,              \n",
    "    stratify = train['label'],\n",
    "    random_state = 42\n",
    ")\n",
    "y_train = train['label'].to_numpy()                  \n",
    "X_train = train.drop(columns = ['label']).to_numpy() \n",
    "X_val = validation.drop(columns = ['label']).to_numpy() \n",
    "y_val = validation['label'].to_numpy()\n",
    "\n",
    "print(f'train : X = {X_train.shape} ; y = {y_train.shape}')\n",
    "print(f'val   : X = {X_val.shape} ; y = {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e190dd-77b5-4e3e-a1e8-aaf760d0261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelForMNISTdigits:\n",
    "    \n",
    "    def __init__(self, learning_rate, iterations, num_of_layers, num_of_neurons, input_size):\n",
    "        if num_of_layers < 2:\n",
    "            raise ValueError(\"Number of layers (nol) must be at least 2 (hidden + output layer).\")\n",
    "        self.nol = num_of_layers\n",
    "        self.non = num_of_neurons\n",
    "        self.w, self.b = [np.random.randn(self.non[0], input_size)], [np.random.randn(self.non[0], 1)]\n",
    "        self.z, self.a = [None], [None]\n",
    "        for i in range(1, self.nol):\n",
    "            self.w.append(np.random.randn(self.non[i], self.non[i-1])*0.1)\n",
    "            self.b.append(np.random.randn(self.non[i], 1))\n",
    "            self.z.append(None)\n",
    "            self.a.append(None)\n",
    "        self.alpha = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "        \n",
    "    def relu_dash(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "        \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis = 0, keepdims = True))\n",
    "        return exp_z / np.sum(exp_z, axis = 0, keepdims = True)\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        self.z[0] = self.w[0].dot(X.T) + self.b[0]\n",
    "        self.a[0] = self.relu(self.z[0])\n",
    "        for i in range(1, self.nol - 1):\n",
    "            self.z[i] = self.w[i].dot(self.a[i-1]) + self.b[i]\n",
    "            self.a[i] = self.relu(self.z[i])\n",
    "        self.z[self.nol - 1] = self.w[self.nol - 1].dot(self.a[self.nol - 2]) + self.b[self.nol - 1]\n",
    "        self.a[self.nol - 1] = self.softmax(self.z[self.nol - 1])\n",
    "\n",
    "    def one_hot(self, y):\n",
    "        one_hot_y = np.zeros((y.size, 10))\n",
    "        one_hot_y[np.arange(y.size), y] = 1\n",
    "        return one_hot_y.T\n",
    "\n",
    "    def back_prop(self, X, y):\n",
    "        m = y.size\n",
    "        onehot_y = self.one_hot(y)\n",
    "        dw, db = [None]*self.nol, [None]*self.nol\n",
    "        dz = self.a[-1] - onehot_y\n",
    "        dw[-1] = (1/m)*dz.dot(self.a[-2].T)\n",
    "        db[-1] = (1/m)*np.sum(dz, axis = 1, keepdims = True)\n",
    "        for i in reversed(range(1, self.nol - 1)):\n",
    "            dz = self.w[i+1].T.dot(dz)*self.relu_dash(self.z[i])\n",
    "            dw[i] = (1/m)*dz.dot(self.a[i-1].T)\n",
    "            db[i] = (1/m)*np.sum(dz, axis = 1, keepdims = True)\n",
    "        dz = self.w[1].T.dot(dz)*self.relu_dash(self.z[0])\n",
    "        dw[0] = (1/m)*dz.dot(X)\n",
    "        db[0] = (1/m)*np.sum(dz, axis = 1, keepdims = True)\n",
    "        for i in range(self.nol):\n",
    "            self.w[i] -= self.alpha*dw[i]\n",
    "            self.b[i] -= self.alpha*db[i]\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        self.forward_prop(X)\n",
    "        predictions = np.argmax(self.a[-1], 0)\n",
    "        accuracy = np.sum(predictions == y) / y.size\n",
    "        print(f'Accuracy : {accuracy}')\n",
    "        \n",
    "    def accuracy(self, y, predictions):\n",
    "        return np.sum(predictions == y) / y.size\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for i in range(self.iterations):\n",
    "            self.forward_prop(X)     \n",
    "            self.back_prop(X, y)\n",
    "            if i%20 == 0:\n",
    "                accuracy = self.accuracy(y, np.argmax(self.a[-1], 0))\n",
    "                print(f'{i} iterations done:')\n",
    "                print(f'    Accuracy: {accuracy}')\n",
    "        accuracy = self.accuracy(y, np.argmax(self.a[-1], 0))\n",
    "        print(f'{self.iterations} iterations done:')\n",
    "        print(f'    Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01259b7f-b37d-4391-a465-14382ba465c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iterations done:\n",
      "    Accuracy: 0.13897222222222222\n",
      "20 iterations done:\n",
      "    Accuracy: 0.6515833333333333\n",
      "40 iterations done:\n",
      "    Accuracy: 0.7496666666666667\n",
      "60 iterations done:\n",
      "    Accuracy: 0.7799722222222222\n",
      "80 iterations done:\n",
      "    Accuracy: 0.8094166666666667\n",
      "100 iterations done:\n",
      "    Accuracy: 0.82725\n",
      "120 iterations done:\n",
      "    Accuracy: 0.8398888888888889\n",
      "140 iterations done:\n",
      "    Accuracy: 0.8496666666666667\n",
      "160 iterations done:\n",
      "    Accuracy: 0.8570277777777778\n",
      "180 iterations done:\n",
      "    Accuracy: 0.864\n",
      "200 iterations done:\n",
      "    Accuracy: 0.86925\n",
      "220 iterations done:\n",
      "    Accuracy: 0.8741666666666666\n",
      "240 iterations done:\n",
      "    Accuracy: 0.8784722222222222\n",
      "260 iterations done:\n",
      "    Accuracy: 0.8828055555555555\n",
      "280 iterations done:\n",
      "    Accuracy: 0.8853611111111112\n",
      "300 iterations done:\n",
      "    Accuracy: 0.8875277777777778\n",
      "320 iterations done:\n",
      "    Accuracy: 0.8899444444444444\n",
      "340 iterations done:\n",
      "    Accuracy: 0.8925\n",
      "360 iterations done:\n",
      "    Accuracy: 0.8942777777777777\n",
      "380 iterations done:\n",
      "    Accuracy: 0.8960277777777778\n",
      "400 iterations done:\n",
      "    Accuracy: 0.8978333333333334\n",
      "420 iterations done:\n",
      "    Accuracy: 0.8996944444444445\n",
      "440 iterations done:\n",
      "    Accuracy: 0.9011944444444444\n",
      "460 iterations done:\n",
      "    Accuracy: 0.90275\n",
      "480 iterations done:\n",
      "    Accuracy: 0.9039722222222222\n",
      "500 iterations done:\n",
      "    Accuracy: 0.9053333333333333\n",
      "520 iterations done:\n",
      "    Accuracy: 0.9070555555555555\n",
      "540 iterations done:\n",
      "    Accuracy: 0.9084722222222222\n",
      "560 iterations done:\n",
      "    Accuracy: 0.90975\n",
      "580 iterations done:\n",
      "    Accuracy: 0.9108888888888889\n",
      "600 iterations done:\n",
      "    Accuracy: 0.9120833333333334\n",
      "620 iterations done:\n",
      "    Accuracy: 0.91325\n",
      "640 iterations done:\n",
      "    Accuracy: 0.9146388888888889\n",
      "660 iterations done:\n",
      "    Accuracy: 0.9159444444444444\n",
      "680 iterations done:\n",
      "    Accuracy: 0.9171388888888888\n",
      "700 iterations done:\n",
      "    Accuracy: 0.9179722222222222\n",
      "720 iterations done:\n",
      "    Accuracy: 0.919\n",
      "740 iterations done:\n",
      "    Accuracy: 0.9197222222222222\n",
      "760 iterations done:\n",
      "    Accuracy: 0.9204166666666667\n",
      "780 iterations done:\n",
      "    Accuracy: 0.92125\n",
      "800 iterations done:\n",
      "    Accuracy: 0.9219166666666667\n",
      "820 iterations done:\n",
      "    Accuracy: 0.923\n",
      "840 iterations done:\n",
      "    Accuracy: 0.9235833333333333\n",
      "860 iterations done:\n",
      "    Accuracy: 0.9246666666666666\n",
      "880 iterations done:\n",
      "    Accuracy: 0.9254444444444444\n",
      "900 iterations done:\n",
      "    Accuracy: 0.9259166666666667\n",
      "920 iterations done:\n",
      "    Accuracy: 0.92675\n",
      "940 iterations done:\n",
      "    Accuracy: 0.9273333333333333\n",
      "960 iterations done:\n",
      "    Accuracy: 0.928\n",
      "980 iterations done:\n",
      "    Accuracy: 0.9289444444444445\n",
      "1000 iterations done:\n",
      "    Accuracy: 0.9298333333333333\n",
      "1020 iterations done:\n",
      "    Accuracy: 0.9305833333333333\n",
      "1040 iterations done:\n",
      "    Accuracy: 0.931\n",
      "1060 iterations done:\n",
      "    Accuracy: 0.9315\n",
      "1080 iterations done:\n",
      "    Accuracy: 0.9320833333333334\n",
      "1100 iterations done:\n",
      "    Accuracy: 0.9326111111111111\n",
      "1120 iterations done:\n",
      "    Accuracy: 0.9329722222222222\n",
      "1140 iterations done:\n",
      "    Accuracy: 0.93375\n",
      "1160 iterations done:\n",
      "    Accuracy: 0.9343333333333333\n",
      "1180 iterations done:\n",
      "    Accuracy: 0.9350555555555555\n",
      "1200 iterations done:\n",
      "    Accuracy: 0.9357777777777778\n",
      "1220 iterations done:\n",
      "    Accuracy: 0.9363055555555555\n",
      "1240 iterations done:\n",
      "    Accuracy: 0.937\n",
      "1260 iterations done:\n",
      "    Accuracy: 0.9374166666666667\n",
      "1280 iterations done:\n",
      "    Accuracy: 0.9375833333333333\n",
      "1300 iterations done:\n",
      "    Accuracy: 0.9381111111111111\n",
      "1320 iterations done:\n",
      "    Accuracy: 0.9383888888888889\n",
      "1340 iterations done:\n",
      "    Accuracy: 0.9390277777777778\n",
      "1360 iterations done:\n",
      "    Accuracy: 0.93975\n",
      "1380 iterations done:\n",
      "    Accuracy: 0.9403333333333334\n",
      "1400 iterations done:\n",
      "    Accuracy: 0.94075\n",
      "1420 iterations done:\n",
      "    Accuracy: 0.9411388888888889\n",
      "1440 iterations done:\n",
      "    Accuracy: 0.9416388888888889\n",
      "1460 iterations done:\n",
      "    Accuracy: 0.9421111111111111\n",
      "1480 iterations done:\n",
      "    Accuracy: 0.9425277777777777\n",
      "1500 iterations done:\n",
      "    Accuracy: 0.9429722222222222\n",
      "1520 iterations done:\n",
      "    Accuracy: 0.9434166666666667\n",
      "1540 iterations done:\n",
      "    Accuracy: 0.9438055555555556\n",
      "1560 iterations done:\n",
      "    Accuracy: 0.9441388888888889\n",
      "1580 iterations done:\n",
      "    Accuracy: 0.9444166666666667\n",
      "1600 iterations done:\n",
      "    Accuracy: 0.9448611111111112\n",
      "1620 iterations done:\n",
      "    Accuracy: 0.9451666666666667\n",
      "1640 iterations done:\n",
      "    Accuracy: 0.9454166666666667\n",
      "1660 iterations done:\n",
      "    Accuracy: 0.94575\n",
      "1680 iterations done:\n",
      "    Accuracy: 0.9460555555555555\n",
      "1700 iterations done:\n",
      "    Accuracy: 0.9465\n",
      "1720 iterations done:\n",
      "    Accuracy: 0.9466944444444444\n",
      "1740 iterations done:\n",
      "    Accuracy: 0.947\n",
      "1760 iterations done:\n",
      "    Accuracy: 0.9472777777777778\n",
      "1780 iterations done:\n",
      "    Accuracy: 0.9475833333333333\n",
      "1800 iterations done:\n",
      "    Accuracy: 0.9478055555555556\n",
      "1820 iterations done:\n",
      "    Accuracy: 0.9480555555555555\n",
      "1840 iterations done:\n",
      "    Accuracy: 0.9483888888888888\n",
      "1860 iterations done:\n",
      "    Accuracy: 0.9487777777777778\n",
      "1880 iterations done:\n",
      "    Accuracy: 0.949\n",
      "1900 iterations done:\n",
      "    Accuracy: 0.9493333333333334\n",
      "1920 iterations done:\n",
      "    Accuracy: 0.9495277777777777\n",
      "1940 iterations done:\n",
      "    Accuracy: 0.9496666666666667\n",
      "1960 iterations done:\n",
      "    Accuracy: 0.9498888888888889\n",
      "1980 iterations done:\n",
      "    Accuracy: 0.9500555555555555\n",
      "2000 iterations done:\n",
      "    Accuracy: 0.9503888888888888\n",
      "2020 iterations done:\n",
      "    Accuracy: 0.9506111111111111\n",
      "2040 iterations done:\n",
      "    Accuracy: 0.9508055555555556\n",
      "2060 iterations done:\n",
      "    Accuracy: 0.9512777777777778\n",
      "2080 iterations done:\n",
      "    Accuracy: 0.95175\n",
      "2100 iterations done:\n",
      "    Accuracy: 0.9519166666666666\n",
      "2120 iterations done:\n",
      "    Accuracy: 0.9520555555555555\n",
      "2140 iterations done:\n",
      "    Accuracy: 0.9523888888888888\n",
      "2160 iterations done:\n",
      "    Accuracy: 0.9525833333333333\n",
      "2180 iterations done:\n",
      "    Accuracy: 0.9527777777777777\n",
      "2200 iterations done:\n",
      "    Accuracy: 0.9531111111111111\n",
      "2220 iterations done:\n",
      "    Accuracy: 0.9534444444444444\n",
      "2240 iterations done:\n",
      "    Accuracy: 0.9536388888888889\n",
      "2260 iterations done:\n",
      "    Accuracy: 0.9537777777777777\n",
      "2280 iterations done:\n",
      "    Accuracy: 0.9538888888888889\n",
      "2300 iterations done:\n",
      "    Accuracy: 0.9542222222222222\n",
      "2320 iterations done:\n",
      "    Accuracy: 0.9543333333333334\n",
      "2340 iterations done:\n",
      "    Accuracy: 0.9543055555555555\n",
      "2360 iterations done:\n",
      "    Accuracy: 0.9545833333333333\n",
      "2380 iterations done:\n",
      "    Accuracy: 0.95475\n",
      "2400 iterations done:\n",
      "    Accuracy: 0.9551388888888889\n",
      "2420 iterations done:\n",
      "    Accuracy: 0.9555\n",
      "2440 iterations done:\n",
      "    Accuracy: 0.9555833333333333\n",
      "2460 iterations done:\n",
      "    Accuracy: 0.9555833333333333\n",
      "2480 iterations done:\n",
      "    Accuracy: 0.9556944444444444\n",
      "2500 iterations done:\n",
      "    Accuracy: 0.9559166666666666\n",
      "2520 iterations done:\n",
      "    Accuracy: 0.9560277777777778\n",
      "2540 iterations done:\n",
      "    Accuracy: 0.9561388888888889\n",
      "2560 iterations done:\n",
      "    Accuracy: 0.9565277777777778\n",
      "2580 iterations done:\n",
      "    Accuracy: 0.9566666666666667\n",
      "2600 iterations done:\n",
      "    Accuracy: 0.9568055555555556\n",
      "2620 iterations done:\n",
      "    Accuracy: 0.9569722222222222\n",
      "2640 iterations done:\n",
      "    Accuracy: 0.9571388888888889\n",
      "2660 iterations done:\n",
      "    Accuracy: 0.9575\n",
      "2680 iterations done:\n",
      "    Accuracy: 0.9578333333333333\n",
      "2700 iterations done:\n",
      "    Accuracy: 0.9579722222222222\n",
      "2720 iterations done:\n",
      "    Accuracy: 0.9582222222222222\n",
      "2740 iterations done:\n",
      "    Accuracy: 0.9582777777777778\n",
      "2760 iterations done:\n",
      "    Accuracy: 0.9582777777777778\n",
      "2780 iterations done:\n",
      "    Accuracy: 0.9583888888888888\n",
      "2800 iterations done:\n",
      "    Accuracy: 0.9585555555555556\n",
      "2820 iterations done:\n",
      "    Accuracy: 0.9588888888888889\n",
      "2840 iterations done:\n",
      "    Accuracy: 0.9591388888888889\n",
      "2860 iterations done:\n",
      "    Accuracy: 0.9594166666666667\n",
      "2880 iterations done:\n",
      "    Accuracy: 0.9596944444444444\n",
      "2900 iterations done:\n",
      "    Accuracy: 0.9598611111111112\n",
      "2920 iterations done:\n",
      "    Accuracy: 0.9600555555555556\n",
      "2940 iterations done:\n",
      "    Accuracy: 0.9602222222222222\n",
      "2960 iterations done:\n",
      "    Accuracy: 0.9604166666666667\n",
      "2980 iterations done:\n",
      "    Accuracy: 0.9605833333333333\n",
      "3000 iterations done:\n",
      "    Accuracy: 0.9603055555555555\n"
     ]
    }
   ],
   "source": [
    "model = ModelForMNISTdigits(learning_rate = 0.05, iterations = 3000, num_of_layers = 3, num_of_neurons = [128, 64, 10], input_size = 784)\n",
    "model.train(X_train / 255, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0d8a88-253c-4117-9e23-41c46dd6d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.923\n"
     ]
    }
   ],
   "source": [
    "model.predict(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227724e0-3b71-42fd-860a-73f348375447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
